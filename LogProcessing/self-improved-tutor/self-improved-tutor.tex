% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - May 2012


\documentclass{acmlarge-edm}

% Metadata Information
\acmVolume{1}
\acmNumber{2}
\acmArticle{3}
\acmYear{2012}
\acmMonth{5}  % can also put in literal name

\usepackage{amsmath}
\usepackage{graphics}

%
%  Cool LaTeX resource:
%   http://en.wikibooks.org/wiki/LaTeX

\begin{document}

% Title portion
\title{Self-improving tutor system}
\author{Brett van de Sande
\affil{Arizona State University\\bvds@asu.edu}}

% Assignment of blame paper: 
% http://www.public.asu.edu/~kvanlehn/Stringent/PDF/07UM_KVL_KK_etal.pdf
%   my ``opportunity'' equals their ``step''
%   my ``turn'' equals their ``transaction''
%
% Doing reinforcement learning by direct policy search.
% See discussion on p. 17 of:
% http://www.cs.brown.edu/research/pubs/theses/phd/2002/peshkin.pdf



\begin{abstract}
Both human and computer tutors constantly make decisions about
what kind of help they are going to give (or not give).  Ideally,
they make these decisions based on some determination of how
the student is progressing coupled with some knowledge of what kind
of tutoring strategies have worked in similar situations in the past.
Thus, we have implemented a method for iteratively improving 
help-giving policies of a computer tutor for introductory physics.  
We created a version of the tutor that randomly uses one of several 
(reasonable) policies when helping students and deployed it in the
classroom.  We then used the resulting student log data to 
train a new version of the tutor having improved hinting policies
and deployed the new version of the tutor in the classroom.  
We discuss what is needed to make this into a sustainable iterative process.
\end{abstract}

\keywords{data mininig, models of student learning}

\begin{bottomstuff}
Funding for this research was provided by the Pittsburgh Science of
Learning Center which is funded by the National Science Foundation
award No. SBE-0836012.
\end{bottomstuff}

\maketitle


\section{Introduction1}


Intelligent tutor systems (ITS) have become increasingly sophisticated
over the years.  Interaction with the students is slowly evolving
from multiple-choice input with static feedback [OLI? CTAT] to more 
sophisticated forms of interaction.  Now days, one can find systems with 
free-form drawing interfaces [Andes], and interactive dialogs [cite Grasser,
Cordillera, etc.].  Along with the more sophisticated interactions
with the student, comes a larger, more complicated set of 
decisions the ITS must make:
%
\begin{itemize}
   \item Do I tell the student what to do next or remain silent?

   \item Do I give a hint or wait until the student asks?

   \item Do I give a pointing hint or do I just tell the student 
         what to do next?

   \item Did the student really learn how to do this or should
         I suggest more practice?

   \item Do I show them a video, or do I just tell them something?
         Should I put up a modal dialog box, so they can't miss it?

   \item Should I ask the student a question to determine their 
         current affect/understanding?

\end{itemize}
%
The traditional way to answer such questions is to conduct
a series of studies, either in the lab or {\em in vivo}.  
In the Learning Sciences, good 
experimental technique requires one or more variable to be
varied while the remaining context be held constant as much as 
possible.  However, as ITS become more sophisticated, the number of 
canditate variables increases and the context becomes more and 
more complicated, and the number of experiments that one needs
to run in order to determine an optimal tutoring strategy becomes
unfeasable. 

\section{Introduction2}

Conventional theories of learning can tell us what kind of help-giving
strategies may be effective and, broadly, when they might be effective.
Likewise, theories of learning may help us identify what kinds of 
features might be useful for determining the student's ``state'' at
a given moment.  The purpose of this project is to fill in the gap
by determining exactly what strategy the tutor should use in a 
particular situation.
The task of finding an optimal help-giving strategy for a
given student state is commonly known as ``Reinforcement Learning.''
We use past student behavior in response to various help-giving
actions on the part of the tutor to construct a reward function
and use that reward function to determine an optimal hinting policy.

Since the agent must give hints in real time, the student state
must contain features that are available in real time, so that
the learned policy can be applied immediately.  In contrast,
the reward function uses subsequent student actions and is only
available after the fact.  Thus, there must be a two-phase process:
an ``off-line'' learning phase to determine the optimal help-giving 
policy and an ``online'' tutoring phase where the policy is applied.



% No action-value function...

\section*{To do:}

Experimental design:  two conditions: normal Andes and learned
help policy.  For each homework, randomly Divide students into two 
even groups.  Reward for experiment is number of incorrect turns
for each KC while that policy is being enforced.



\section{Models of learning}

A number of authors have used a logistic function, 
$\mathrm{logit}(x)=1/\left(1+e^{-x}\right)$, to 
model student learning of a KC.  In that case, we have a two-parameter
model for the probability that the student gets step $j$ correct:
%
\begin{equation}
               P_j^\mathrm{logit} = \mathrm{logit}\left(\beta (j-L)\right)
\end{equation}
%
where $\beta$ is the learning rate and $L$ is 
the the step where the probability of correctness has risen to 50\%.

Alternatively, we can use a function that retains the 
guess $P(G)$ and slip $P(S)$ rates of Corbett and 
Anderson~\cite{corbett_knowledge_1994}.
The simplest such model is a step-function:
%
\begin{equation}
               P_j^\mathrm{step} = \left\{\begin{array}{cc}
                                       g,& j<L\\
				       1-s,& j\ge L
                                    \end{array}\right. \label{step}
\end{equation}
%
where $g$ is the guess rate, $s$ is the slip rate and 
$L$ is ``the moment of learning,'' the step where the student
first shows mastery of the KC; see Figure~\ref{stepf}.
For a given $L$, we can define an average learning gain,
%
\begin{equation}
     % need actual formula here.
         \Delta_L = 1- \left\langle g+s \right\rangle_L ,
\end{equation}
%
where $\left\langle \cdots \right\rangle_L$ denotes the
expecation value.
\begin{figure}
\centering    \includegraphics{step.eps}
\caption{The step model with $g=0.15$, $s=0.8$, and $L=4$.}
         \label{stepf}
\end{figure}
%
We can fit $P_j^\mathrm{step} $ to the student data using the
Maximum Likelihood method.
Let $c_i$ \& $w_i$ be the number of correct \& incorrect steps observed
for steps $j<L$ and $c_f$ \& $w_f$ be the number of correct \& incorrect
steps for steps $j\ge L$.  The associated log likelihood for a given $L$ is
%
\begin{equation}
  \mathcal{L}_L  =  \log\left(B_{c_i,w_i}(g)\right) 
                  + \log\left(B_{c_f,w_f}(1-s) \right)  \, ,
		  \label{logLike}
\end{equation}
%
where the guess rate $g$ and slip rate $s$ follow the binomial distribution,
%
\begin{equation}
       B_{a,b}(x) = x^a (1-x)^b\; \frac{(x+b+1)!}{a!\, b!} 
\end{equation}
%
with normalization
%
\begin{equation}
      1=\int_0^1 B_{a,b}(x) \,\mathrm{d}x \; .
\end{equation} 
%
The maximum-likelihood estimators (MLE) for $g$ and $s$ are
%
\begin{eqnarray}
  \hat{g} &=&  \frac{c_i}{c_i+w_i} \, ,\\
  \hat{s} &=&  \frac{w_f}{c_f+w_f} \,  .
\end{eqnarray}
%
Similarly, one can find the maximum-likelihood estimator
for $L$.  However, in some cases, $L$ is not normally
distributed and has a large uncertainty.
So, instead of using the MLE value of $L$ and some
associated error, we will use a multimodel approach~\cite{burnham_model_2002}.
We will use a set of models of the form (\ref{step}), 
one for each $L$. Each model has a relative probability given 
by its Akaike weight,  
%
\begin{equation}
                   w_L = \frac{\mathrm{e}^{\mathcal{L}_L-2}}{W}\, ,
\end{equation}
%
with normalization $W$ such that $\sum_L w_L=1$.

%
%  This is not really needed below (remove later)
%
%Finally, we may want to determine whether, in the context
%of the step-function model (\ref{step}), learning has occurred.
%We say that learning has occurred at $L$ if $g<1-s$.
%Since the model is a fit to student data ($c_i$, $w_i$, $c_f$, $w_f$), 
%we can, at best, determine the {\em probability} that learning has occurred. 
%The probability that learning has occurred (for a given L) 
%can be calculated by integrating the Likelihood (see Eqn.~(\ref{logLike})) 
%over the possible slip and guess rates in the region $g+s<1$:
%
%\begin{equation}
%   P(c_i, w_i, c_f, w_f)= \int_0^1 \int_0^{1-g} 
%   B_{c_i,w_i}(g) \, B_{c_f,w_f}(1-s) \,\mathrm{d}g\,\mathrm{d}s
%\end{equation}
%%
%This integral can be solved exactly by repeated integration by parts.

\section{Self-improved tutor system}

\begin{figure}
\centering    \includegraphics{process.eps}
\caption{Iterative process for improvement of help-giving policies in a
  tutor system.} \label{process}
\end{figure}



\section{Objective function}

The basic approach for solving this problem, in terms of reinforcement
learning, is called a ``direct policy search.''
For each student, help-giving policy, and KC, we define the objective 
function to be 
%
\begin{subequations}
  \label{objective}
  \begin{align}
  Z =& \sum_L w_L \left\{\sum_{j \in \mathcal{I},\,j<L}  
  % Actually, we use the number of incorrect steps
  % between j and L for the discount factor.
  % need to add this to formula.
       \frac{\gamma^{n(j,L)} \Delta_L}{\left|\sigma_j\right|}
  \sum_{k\in \sigma_j} \left(f(\mathbf{x}_k)-d_k\right))^2 \right.
     \label{before} \\
 % reward for after-learning policy
 % Includes case with no learning as k=0.
  &+\beta \left. \sum_{j \in \mathcal{I},\,j \ge L} \frac{1-\hat{s}}
      {n(L,j_\mathrm{max})\left|\sigma_j\right|}
             \sum_{k\in \sigma_j} \left(f(\mathbf{x}_k)-d_k\right))^2
     \right\}
  \label{after}
  \end{align}
\end{subequations}
%
where $L$ labels different models, each with weight $w_L$;
$\mathcal{I}$ is the set of steps associated
with that student, policy, and KC that were marked ``incorrect.''  
$\sigma_j$ is the set of transactions associated with step $j$,
$f(\mathbf{x}_k)$ is the machine-learned policy associated
with transaction $k$ and
$d_k\in \{0,1\}$ is the
policy actually taken by the random-help version of Andes for that 
transaction.
We have arranged things so that finding an optimal policy 
corresponds to {\em minimizing} $Z$.

The first term (\ref{before}) rewards policies applied directly before
the moment of learning (or step $j=L$ for model $L$).  
These are weighted by the 
average learning gain $\Delta_L = 1-\left\langle g+s\right\rangle_L$ 
predicted by model $L$.
Also, $n(j,L)$ is the number
of incorrect steps $\mathcal{I}$ between $j$ and $L$, 
and $\gamma$ is the ``discount factor''~\cite{russell_artificial_2002}. 
A good value for $\gamma \in [0,1]$ must be determined empirically.
In a previous study involving physics learning~\cite{chi_empirically_2011}, 
$\gamma=0.9$ was chosen.
However, in that case, the discount factor was applied transaction-wise in 
a situation where there were many transactions per step.  In our case, 
we apply $\gamma$ step-wise to incorrect steps $\mathcal{I}$, 
suggesting a lower number might be appropriate.

Also, the tutor must have a strategy to apply in the case of slips: 
errors that occur after the student has learned the KC.
The second term (\ref{after}) rewards policies applied after
the KC has been learned.  Since the only available measure of 
effectiveness after the moment of learning is the slip rate $\hat{s}$, 
we look for a policy that maximizes $1-\hat{s}$.  The reward for 
this policy is evenly divided among the $n(L,j_\mathrm{max})$ 
incorrect steps after $L$.
Since our primary objective is to produce learning in the first
place (\ref{before}), we want the contribution of (\ref{after})
to $Z$ to be somewhat smaller.  Thus, we choose $\beta$ numerically 
such that the contribution of (\ref{after}) is somewhat smaller than 
the contribution of (\ref{before}).

The machine learning algorithm finds a function $f$ that acts on
the set of states $\left\{\mathbf{x}_k\right\}$ that minimizes
the objective function $Z$ summed overs students and KC's.  
Since our policies are binary-valued
and many of our features are well ordered (times, counts of transactions,
{\em et cetera}), it is natural to define $f$ in terms of a 
linear classifier.  Thus
%
\begin{equation}
              f(\mathbf{x}_k) = \left\{\begin{array}{cc}
		1,& \mathbf{a}\cdot \mathbf{x}_k <b \\
                0, & \mathbf{a}\cdot \mathbf{x}_k \ge b
		\end{array} \right.
\end{equation}
%
That is, $\mathbf{a}$ and $b$ define a hyperplane in feature
space.  All states on one side of the plane are given policy 0
and states on the other side have policy 1.
Numerically, we find $\mathbf{a}$ and $b$ that minimizes $Z$
summed over students and KC's.

\section{Feature space}

The student state at any given time is described by a list of
features.   Since the machine learned policy is to act on this list of
features, they must be computable in real time.  This is can be a
challenge in tutor system with a free-form interface like  Andes:
there is not always a way to determine, in real time, what a student was
attempting when they make an entry and get it wrong, even if the attempt
is examined by an expert human tutor.  Andes has a
number of error handlers that do make such a determination, but they 
apply only for certain well-known errors.

For determining which features to select, we used Min Chi's thesis
as a starting point~\citeyear{chi_micro-level_2009}, 
although she was doing this for a
rather different kind of tutor system.  The following items were
chosen mostly for convenience, and they avoided issues with determing
the KC associated with an entry.
%
\begin{description}

\item[logSessionTime] Log of number of seconds since start of current
  session.  This is a measure of student fatigue.

\item[fracSessionFlounderTime] Flounder time is the time spent between 
incorrect (object turns red) transactions, with no intervening correct 
(object turns green) transactions. 
 {\em fracSessionFlounderTime} is the total flounder
time for the current session over total session time.

\item[logNowRedTrans] Log of number of transactions since last
  correct (object turns green) transaction.

\item[logNowRedTime] Log of number of seconds since last
  correct (object turns green) transaction.

\item[logNowIdleTime] Log of number of seconds since last transaction.

\item[sessionCorrect] Number of objects in current session that have
  been  been marked  correct.

\item[sessionIncorrect] Number of transactions in current session where 
   object has been marked  incorrect.

\item[sessionHelp] Number of help requests in current session.

\item[fracSessionCorrect] $\mbox{sessionCorrect}/\left(\mbox{sessionCorrect}+\mbox{sessionIncorrect}+\mbox{sessionHelp}\right)$.

\end{description}
%
We have taken the logarithm of quantities where the student data
for that quantity ranges over several orders of magnitude.

On the Andes help server, the state for a given transaction 
is determined before the student action itself is analyzed,
in order that the help policy for that transaction can be set.  
Thus the state can only depend correct/incorrect (or floundering) 
information from previous transactions. 


% Bibliography
\bibliographystyle{acmlarge}
\bibliography{education-modeling}

\end{document}