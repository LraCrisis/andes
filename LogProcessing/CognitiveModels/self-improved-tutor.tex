\documentclass[11pt,letterpaper]{article}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{fullpage}
\addtolength{\voffset}{0.5in}

%
%  Cool LaTeX resource:
%   http://en.wikibooks.org/wiki/LaTeX

\begin{document}
\title{Self-improving tutor system}
\author{Brett van de Sande}

% Assignment of blame paper: 
% http://www.public.asu.edu/~kvanlehn/Stringent/PDF/07UM_KVL_KK_etal.pdf
%   my ``opportunity'' equals their ``step''
%   my ``turn'' equals their ``transaction''
%
% Doing reinforcement learning by direct policy search.
% See discussion on p. 17 of:
% http://www.cs.brown.edu/research/pubs/theses/phd/2002/peshkin.pdf


\maketitle

\section{A model of learning}

For each student and KC, the student has attempted some number of 
{\em steps} that involve a given skill.   We will label
steps with $j$.  Usually a given step is associated
with a single user interface object (an equation, vector, etc.)  but
not always, since a student may attempt a particular problem solving
step, delete the object, and later attempt that solution step again.
Each step $j$ corresponds some some number of student {\em transactions}:
attempts at constructing the associated object, or associated
interactions with the Andes help system.  

Next, we need a model of student learning for a particular KC.
Since the policies chosen by the random-help version of Andes
are different for each student,
we need to determine the point of learning for each student.
For each KC and student, mark each step as ``correct'' if
the student completes the step correctly without any associated errors or 
requests for help; otherwise, the step is marked as ``incorrect.''

\section{Models of learning}

A number of authors have used a logistic function, 
$\mathrm{logit}(x)=1/\left(1+e^{-x}\right)$, to 
model student learning.  In that case, we have a two-parameter
model for the probability that the student gets step $j$ correct is:
%
\begin{equation}
               P_j^\mathrm{logit} = \mathrm{logit}\left(\beta (j-L)\right)
\end{equation}
%
where $\beta$ is the learning rate and $L$ is the ``moment
of learning''~\cite{aha}.

Alternatively, we can use a function that retains the 
guess $P(G)$ and slip $P(S)$ probabilities of Corbett and 
Anderson~\cite{anderson}.
The simplest such model is a step-function:
%
\begin{equation}
               P_j^\mathrm{step} = \left\{\begin{array}{cc}
                                       g,& j<L\\
				       1-s,& j\ge L
                                    \end{array}\right. \label{step}
\end{equation}
where $g$ is the guess rate, $s$ is the slip rate and 
$L$ is the moment of learning.
%
\[
\includegraphics{step.eps}
\]
%
We can match $P_j^\mathrm{step} $ to the student data using the
Maximum Likelihood method.
Let $c_i$ \& $w_i$ be the number of correct \& incorrect steps observed
for steps $j<L$ and $c_f$ \& $w_f$ be the number of correct \& incorrect
steps for steps $j\ge L$.  The associated log likelihood for a given $L$ is
%
\begin{equation}
  \mathcal{L}_L  =  \log\left(B_{c_i,w_i}(g)\right) 
                  + \log\left(B_{c_f,w_f}(1-s) \right)  \, ,
\end{equation}
%
where the guess rate $g$ and slip rate $s$ follow the binomial distribution,
%
\begin{equation}
       B_{a,b}(x) = x^a (1-x)^b\; \frac{(x+b+1)!}{a!\, b!} 
      \;\;\;\; \mbox{where} \;\;\;\;
      1=\int_0^1 B_{a,b}(x) \,\mathrm{d}x \; .
\end{equation} 
%
$\mathcal{L}_L$ is maximized when:
%
\begin{eqnarray}
  g = P(G) &=&  \frac{c_i}{c_i+w_i} \, ,\\
  s = P(S) &=&  \frac{w_f}{c_f+w_f} \,  .
\end{eqnarray}
%
Next, one can maximize $\mathcal{L}_L$ with respect to $L$ to find
the most likely value $L=L_\mathrm{max}$.  However, in practice, the 
uncertainty in $L$ can be significant and may not have a normal
distribution.   Instead, we will consider a set of sub-models, each with a 
different value of $L$, and a relative probability given by 
AIC~\cite{aic-book}.  
That is, the sub-model for a particular value of $L$ has weight,
%
\begin{equation}
                   w_L = \frac{\mathrm{e}^{\mathcal{L}_L-2} }{W}\, ,
\end{equation}
%
with normalization $W$ such that $\sum_L w_L=1$.
For each sub-model, we can define an average learning gain,
%
\begin{equation}
     % need actual formula here.
         \Delta_L = 1- \left\langle g+s \right\rangle_L \; .
\end{equation}

Finally, we need to determine whether, in the context
of the step-function model (\ref{step}), learning has occurred.
We say that learning has occurred at $L$ if $g<1-s$.
Since the model is a fit to student data ($c_i$, $w_i$, $c_f$, $w_f$), 
we can, at best, determine the {\em probability} that learning has occurred. 
The probability that learning has occurred can be calculated by
integrating over the possible slip and guess rates in the region
$p+g<1$, weighted by the probability distributions:
%
\begin{equation}
   P(c_i, w_i, c_f, w_f)= \int_0^1 \int_0^{1-g} 
   B_{c_i,w_i}(g) \, B_{c_f,w_f}(1-s) \,\mathrm{d}g\,\mathrm{d}g 
\end{equation}
%
This integral can be solved exactly by repeated integration by parts.


\section{Objective function}

The basic approach for solving this problem, in terms of reinforcement
learning, is called a ``direct policy search.''
For each student, help-giving policy, and KC, we define the objective 
function to be 
%
\begin{subequations}
  \label{objective}
  \begin{align}
  Z =& \sum_L w_L \left\{\sum_{j \in \mathcal{I},\,j<L}  
  % Actually, we use the number of incorrect steps
  % between j and L for the discount factor.
  % need to add this to formula.
       \frac{\gamma^{n(j,L)} \Delta_L}{\left|\sigma_j\right|}
  \sum_{k\in \sigma_j} \left(f(\mathbf{x}_k)-d_k\right))^2 \right.
     \label{before} \\
 % reward for after-learning policy
 % Includes case with no learning as k=0.
  &+\beta \left. \sum_{j \in \mathcal{I},\,j \ge L} \frac{(1-P(S))}
      {n(L,j_\mathrm{max})\left|\sigma_j\right|}
             \sum_{k\in \sigma_j} \left(f(\mathbf{x}_k)-d_k\right))^2
     \right\}
  \label{after}
  \end{align}
\end{subequations}
%
where $L$ labels different sub-models, each with weight $w_L$;
$\mathcal{I}$ is the set of steps associated
with that student, policy, and KC that were marked ``incorrect.''  
$\sigma_j$ is the set of transactions associated with step $j$,
$f(\mathbf{x}_k)$ is the machine-learned policy associated
with transaction $k$ and
$d_k\in \{0,1\}$ is the
policy actually taken by the random-help version of Andes for that 
transaction.
We have arranged things so that finding an optimal policy 
corresponds to {\em minimizing} $Z$.

The first term (\ref{before}) rewards policies applied directly before
the moment of learning (or step $L$ for sub-model $L$).  
These are weighted by the 
average learning gain $\Delta_L = 1-\left\langle g+s\right\rangle_L$ 
predicted by sub-model $L$.
Also, $n(j,L)$ is the number
of $\mathcal{I}$ between $j$ and $L$, 
and $\gamma$ is the ``discount factor''~\cite{ml}. 
A good value for $\gamma \in [0,1]$ must be determined empirically.
In a previous study~\cite{mint}, $\gamma=0.9$ was chosen.
However, the discount factor was applied transaction-wise in 
a case where there were many transactions per step.  In our case, 
we apply $\gamma$ to incorrect steps $\mathcal{I}$, suggesting a lower
number might be appropriate.

The second term (\ref{after}) rewards policies applied after
the KC has been learned.  Since the only measure of effectiveness
after is the slip rate $P(S)$, we look for a policy that maximizes
$1-P(s)$.  The reward for an this policy is evenly divided
among the $n(L,j_\mathrm{max})$ incorrect steps after $L$.
Since our primary objective is to produce learning in the first
place (\ref{before}), we want the effect of (\ref{after}) to be 
relatively small.
Thus, the parameter $\beta$ should be small, but nonzero; an
optimum value must be determined empirically.

The machine learning algorithm finds a function $f$ that acts on
the set of states $\left\{\mathbf{x}_k\right\}$ that minimizes
the objective function $Z$ summed overs students and KC's.  
Since our policies are binary-valued
and many of our features are well ordered (times, counts of transactions,
{\em et cetera}), it is natural to define $f$ in terms of a 
linear classifier.  Thus
%
\begin{equation}
              f(\mathbf{x}_k) = \left\{\begin{array}{cc}
		1,& \mathbf{a}\cdot \mathbf{x}_k <b \\
                0 & \mathbf{a}\cdot \mathbf{x}_k \ge b
		\end{array} \right.
\end{equation}
%
That is, $\mathbf{a}$ and $b$ define a hyperplane in feature
space.  All states on one side of the plane are given policy 0
and states on the other side have policy 1.
Numerically, we find $\mathbf{a}$ and $b$ that minimizes $Z$
summed over students and KC's.



\begin{thebibliography}{9}

\bibitem{anderson} 
  Corbett, A.\ T., Anderson, J.\ R. Knowledge Tracing:  Modeling 
the Acquisition of Procedural Knowledge.  \emph{User Modeling and
 User-Adapted Interaction}, 1995, 4, 253--278.

\bibitem{beckchang}
  Beck, J.\ E., Chang, K.-m.\ Identifiability: A Fundamental Problem of
  Student Modeling.
  \emph{Proceedings of the $11^{th}$ International Conference on User 
    Modeling}, 2007.

\bibitem{aic-book}Burnham, K.~P., and Anderson, D.~R. \emph{Model
  Selection and Multimodel Inference: A Practical
  Information-Theoretic Approach}, 2nd ed. Springer-Verlag. 2002.

\bibitem{aha}Kurt and Kasia moment of learning paper.

\bibitem{ml}Discount factor citation.

\bibitem{mint}Min's Thesis (or publication?)

\bibitem{linear}Linear classifier Ref. 
     http://en.wikipedia.org/wiki/Linear\_classifier

\end{thebibliography}




\end{document}