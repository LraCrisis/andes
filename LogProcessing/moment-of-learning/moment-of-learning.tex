% acmsmall-sample.tex, dated 24th May 2012
% This is a sample file for ACM small trim journals
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - May 2012


\documentclass{acmlarge-edm}

% Metadata Information
\acmVolume{1}
\acmNumber{2}
\acmArticle{3}
\acmYear{2012}
\acmMonth{5}  % can also put in literal name

\usepackage{graphics}

% Metadata Information would go here

% Document starts
\begin{document}


% Title portion
\title{Models of student learning and multimodel inference}
\author{Brett van de Sande
\affil{Arizona State University\\bvds@asu.edu}}

%
%  Cool LaTeX resource:
%   http://en.wikibooks.org/wiki/LaTeX

\begin{abstract}
We compare several models of student learning with the goal of 
determining when a specific student has learned a particular skill.  
We use Likelihood Theory to determine
which of several models best describes the acquisition
of skills associated with introductory physics.  Then we
use a multimodel approach to determine the relative probability
that a student has learned a given skill at a particular 
problem-solving step.
\end{abstract}

%\category{C.2.2}{Computer-Communication Networks}{Network Protocols}

%\terms{Design, Algorithms, Performance}

\keywords{data mininig, models of student learning}

%\acmformat{Zhou, G., Wu, Y., Yan, T., He, T., Huang, C., Stankovic,
%J. A., and Abdelzaher, T. F.  2010. A multifrequency MAC specially
%designed for  wireless sensor network applications.}

\begin{bottomstuff}
Funding for this research was provided by the Pittsburgh Science of
Learning Center which is funded by the National Science Foundation
award No. SBE-0836012.
\end{bottomstuff}


\maketitle


\section{Introduction}

%Talk about ultimate goal of using this to determine effectiveness
%of help given or of a particular student behavior.

There is ample evidence that a well-designed Intelligent Tutor 
System (ITS) can be almost as effective as an expert human 
tutor~\cite{vanlehn_relative_2011}.  A key ingredient for improving an ITS is
determining whether a particular student action or the 
tutoring given by the ITS at a given instance was effective.
There are several senses in which an activity might be ``effective.''
For instance:  
%
\begin{itemize}
\item Did it improve the student's attitude toward
the subject?  
\item Did it minimize time-on-task (assuming 
a skill was eventually learned)?  
\item Did it improve the student's 
problem-solving skills?  
\item Did it prepare the student to learn
material later in the course?
\end{itemize}

In the present investigation, we will take a narrower definition
of effectiveness of a particular student or tutor action:  
Did the action immediately precede the
student learning a new skill?  Answering this question requires
the use of some sort of embedded assessment of student learning.
Presumably, a student will encounter a given skill multiple times
during a problem solving session, so any pre-test/post-test will
not tell us directly at what step the student learned the skill
(although the effectiveness of a particular action can
be experimentally inferred by conducting a two-condition study).
In addition, it means that the determination must be made
after the fact, typically through some log file analysis. 

In this work, we investigate a method for determining
when a student may have learned a skill based on their
ability to apply that skill without errors or asking for hints.
In principle, there are other observables that may give us
clues on mastery:  for instance, how much time a student takes
to complete a step.  However, other such observables need
some additional interpretation 
({\em exempli gratia,} How long is too long?).
Baker, Goldstein, and Heffernan~\citeyear{baker_detecting_2010} 
attempt a model of learning based on a Hidden Markov model approach.  In their
model, they consider additional variables.  Inclusion of
such additional observables is a natural extension of
the approach we will present here.

First we will compare three different models of learning
and examine whether there is emphirical support for using
one model over the others.  In fact, using Akaike Information
Criteria (AIC), we obtain results that seem to favor two
models over the third, but conclude that there are some
unresolved issues.

Second, we select a model and use a multimodel approach
to predict where learning has occurred, with what probability,
and how much learning has occurred.  We apply our approach
to student data and discuss the reliability of our predictions.


\subsection{Correct/Incorrect steps}

Although there are other observables that may be used
to determine whether a skill has been mastered, Table~1 of
\cite{baker_detecting_2010} list 14 different observables.  
However, we will focus on correctness of a step.  Thus, we need to
define precisely what we mean by a step being correct.

For each student and Knowledge Component (KC)~\cite{vanlehn_behavior_2006}, 
the student has attempted some number of 
{\em steps} that involve that KC.   We will label
steps with $j$.  Usually a given step is associated
with a single user interface object (an equation, vector, etc.)  but
not always, since a student may attempt a particular problem solving
step, delete the object, and later attempt that solution step again.
Sometimes, we talk of a step being an {\em opportunity} to learn
a given skill if the student needs to apply that skill
to complete that step.

%
%  Not needed in this paper.
%
%Each step $j$ corresponds some some number of student-tutor 
%{\em transactions}: attempts at constructing the associated object, 
%or associated interactions with the Andes help system.  

%Next, we need a model of student learning for a particular KC.
%Since the policies chosen by the random-help version of Andes
%are different for each student,
%we need to determine the point of learning for each student.
For each KC and student, mark each step as ``correct'' if
the student completes the step correctly without any associated errors or 
requests for help; otherwise, the step is marked as ``incorrect.''
\label{steps} 
%
% From Kurt:
Thus, if each incorrect/correct step is marked with a 0/1, then
a single student's performance on a single KC is a bit string,
{\em exempli gratia} 00101011.

\section{Three models of learning}

In order to determine when a student has learned a particular still,
we need to introduce a model of learning for that student and skill.
Ideally, the model should have the the following properties:
\label{model-criteria}
%
\begin{enumerate} 

\item Be compatible with actual student behavior.
      See Section~\ref{model-selection}.  That is, its
      functional form should fit well with student data.

\item \label{crit:step}
      Give the probability that learning has occurred at a given step.
      See Section~\ref{multi-model}.

\item  \label{crit:perform}
      Assuming learning has occurred at a given step, give the 
     associated increase in performance and 
     the rate of errors after learning.

\end{enumerate}
%
We will consider three candidate models:  the ``step model,'' 
the logistic function, and the Bayesian Knowledge Tracing model.


The first model is the Bayesian Knowledge Tracing (BKT) 
model~\cite{corbett_knowledge_1994}.  The hidden Markov model
form of BKT is often fit to student performance 
data~\cite{beck_identifiability:_2007}.  One can show that
this model, in functional form, is an exponential function
with three model parameters~\cite{van_de_sande_bayesian_2012}:
%
%  A and \beta are kind of messy, so we don't include them here.
\begin{equation}
         P_\mathrm{BKT}(j) = 1-P(S) -A e^{-\beta j} \; .
\end{equation}
%
One central assumption of BKT is that, given that learning
has not already occurred, mastery is {\em equally probable} on each step.
This assumption of equal probability does not match well with 
our goal of determining empirically the steps where learning has 
actually occurred for an individual student, criterion~\ref{crit:step}.
On the other hand, this model does provide initial and final
error rates, so criterion~\ref{crit:perform} is satisfied. 

Another model that is frequently used in the context of 
learning is the logistic model~\cite{cen_learning_2006,min_chi_instructional_2011},
%
\begin{equation}
    P_\mathrm{logistic}(j)= \frac{1}{1+\exp\left(-b (j-L)\right)} \; .
\end{equation}
%
It is natural to associate $L$ with the moment of learning.  However,
the finite slope $P_\mathrm{logistic}(j)$ means that learning may occur
in a range of roughly $1/b$ steps before and after $L$.    For  
$P_\mathrm{logistic}(j)$, the gain in performance is always 1 and the final error
rate is always 0.  Thu,s although this model makes a prediction for
when the skill is learned, criterion~\ref{crit:step}, it does
not predict a gain in performance, criterion~\ref{crit:perform}.

The ``step model'' assumes that learning occurs all at 
once~\cite{baker_detecting_2011}.  It is defined as:
%
\begin{equation}
    P_\mathrm{step}(j)= \left\{\begin{array}{cc}
                 g, & j<l \\
                 1-s, & j\ge L 
                 \end{array} \right. 
\end{equation}
%
where $L$ is the step where the student first shows mastery of the
KC, $g$ is the ``guess rate,'' the probability that the student
gets a step correct by accident, and $s$ is the ``slip rate,''
the chance that the student makes an error after learning
the skill.  These are analogous to the guess and slip parameters 
of BKT~\cite{corbett_knowledge_1994}.  
The associated gain in performance
is $1-g-s$ and the error rate after learning is simply $s$ in this
model.  Thus, this model satisfies criteria \ref{crit:step} and
\ref{crit:perform}.

\section{Model selection using AIC}
\label{model-selection}

Although this will not determine our choice of model in subsequent
work, it would be reassuring to know whether the step model 
$P_\mathrm{step}(j)$
describes the student data as well (or better than) the
other two models, $P_\mathrm{logistic}(j)$ and $P_\mathrm{BKT}(j)$.  
We believe that the Akaike Information 
Criterion (AIC) is the most appropriate metric~\cite{akaike_new_1974,burnham_model_2002}
to use for this purpose.

\subsection{Method}


\begin{figure}
  \centering \includegraphics{student-kc-length-histogram.eps}
  \caption{Histogram of number of student-KC pairs in student 
    dataset $\mathcal{A}$ having a given number of steps.}
    \label{student-length-histogram}
\end{figure}

We examined log data from 12 students taking an intensive 
introductory physics course at St.\ Anselm College during
summer 2011.  The course
covered the same content as a normal two-semester introductory
course.  Log data was recorded as students solved homework 
problems while using the Andes intelligent tutor homework system.
231 hours of log data were recorded.
%, covering 85,744 transactions, and 26,204 student steps.  
Each step was assigned to one or more different KC's.  
The dataset contains a total of 2017 distinct
student-KC pairs covering a total of 245 distinct KC's.
We will refer to this dataset as student dataset $\mathcal{A}$.
See Figure~\ref{student-length-histogram} for a histogram
of the number student-KC pairs having a given number of steps.

Most KC's are associated with physics
or relevant math skills while others are associated with 
Andes conventions or user-interface actions (such as, notation
for defining a variable).  The student-KC pairs with the largest 
number of steps are associated with user-interface related skills,
since these skills are exercised throughout the entire course. 

The presence of many student-KC pairs with just one or two
steps may indicate that the default cognitive model associated 
with this tutor system may be sub-optimal; to date, there has not 
been any attempt, to date, to improve on the cognitive model of 
Andes with, say, Learning Factors Analysis~\cite{cen_learning_2006}.

\subsection{Analysis}

Since the goodness of fit criterion, AIC, is valid in the limit 
of many steps, we include in this analysis only student-KC 
pairs that contain 10 or more steps, reducing the number of 
student-KC pairs to 267, covering 38 distinct KC's.
We determine the correctness of each step (Section~\ref{steps}),
constructing a bit string, {\em exempli gratia}
001001101, for each student-KC pair.  
This bit string is then fit to each of the three models,
$P_\mathrm{step}(j)$, $P_\mathrm{logistic}(j)$, and $P_\mathrm{BKT}(j)$ by
maximizing the associated log likelihood.  
%This determines the free parameters in each model. 
We then calculate the corrected AIC score,
 AIC$_\mathrm{c}$~\cite{burnham_model_2002} for each fit.  
Finally, we calculated the Akaike weights, $w_\mathrm{logistic}$,
$w_\mathrm{step}$, and $w_\mathrm{BKT}$ for each student-KC pair.  
The Akaike weight represents the relative probability that
a particular model in a given set of models most closely matches
the true model that has actually generated the data.
The weights are normalized so that 
%
\begin{equation}
   1=w_\mathrm{logistic}+ w_\mathrm{step} + w_\mathrm{BKT} \; .
\end{equation}
%
A scatter plot of the weights is shown in Fig.~\ref{scatter1}.
Since a student-KC pair contains only an average of about 16 steps, we 
expect that AIC$_\mathrm{c}$ should not strongly
discriminate between the models.  Instead, we expect that
any statistically significant difference will only appear 
after including hundreds of student-KC pairs in our sample.  However, 
this is not what we found; instead, we see that $P_\mathrm{BKT}$ is 
strongly disfavored over $P_\mathrm{step}(j)$ and $P_\mathrm{logistic}(j)$.
Perhaps this result is too good to be true?

\begin{figure}
  \centering \includegraphics{scatter-weights.eps}
  \caption{Scatter plot of  Akaike weights for the three models, 
   $P_\mathrm{step}$, $P_\mathrm{logistic}$, and $P_\mathrm{BKT}$, 
   when fit to 267 student-KC pairs from an introductory physics course.
   The model $P_\mathrm{BKT}$ seems to be highly disfavored over
   the other two models; however, we believe this result
   may be spurious.} \label{scatter1}
\end{figure}

To further investigate this situation, we constructed an
artificial dataset containing 100 random bit strings (each
step has 50\% probability of being ``correct'') of 100 steps each.
This dataset corresponds to a model of the form
%
\begin{equation}
        P_\mathrm{random}(j)=1/2 \; .
\end{equation}
%
We then repeated our analysis of the three models using this
dataset.  In this case, one expects that all three models 
should perform equally well since all three can equal 
(with suitable choice of parameters) the known correct model 
$P_\mathrm{random}(j)$.
Thus, we would expect a scatter plot of the Akaike weights to 
center around 
$w_\mathrm{logistic}=w_\mathrm{step}=w_\mathrm{BKT}=1/3$.
Instead, we find that $P_\mathrm{step}$ is highly favored over
the other two; see Fig.~\ref{scatter2}.  
This indicates that there are large non-leading
corrections to the AIC$_\mathrm{c}$ that have not been
taken into account.  Burnham \& Johnson conclude 
that there is more work to be done in this area~\cite[p.~380]{burnham_model_2002}: 
``Operating characteristics
of AIC-based model selection for count-type data need more 
more study for small sample sizes.'' 

\begin{figure}
  \centering \includegraphics{scatter-random-weights.eps}
  \caption{Scatter plot of Akaike weights for the three models, 
   $P_\mathrm{step}$, $P_\mathrm{logistic}$, and $P_\mathrm{BKT}$, 
   when fit to randomly generated data.  For this dataset, each 
   model should perform
   equally well; the large bias in favor of one model indicates
   that  AIC$_\mathrm{c}$ is failing.}\label{scatter2}
\end{figure}

\subsection{Summary}

In conclusion, the most powerful technique for model selection,
AIC, fails to to properly distinguish models when the random
variable (the dataset) is binary-valued (Bernoulli trials),
the sample sizes are small (10 to 100 points), and the models
have very different functional forms.  Since the use of such
models is foundational to KC-based approaches to Educational
Data mining, we believe that it is imperative that this issue 
associated with model selection must be resolved.

Demonstrating that the step model fits student data as well as,
or better than, the other two would be an
{\em ex post facto} justification for using that model.  
However, our use of the step model is primarily motivated
by criteria~\ref{crit:step} and \ref{crit:perform} of 
Section~\ref{model-criteria}.
In Section~\ref{multi-model}, we show how AIC-based methods
can yield the probability that learning has occurred during given step.

\section{Multi-model approach}
\label{multi-model}

We need to determine the step where a specific student has learned a
particular skill.  Our strategy is to take the step model, 
$P_\mathrm{step}(j)$, and treat $L$ as a constant, yielding a set of 
sub-models $P_{\mathrm{step},L}(j)$.
We then fit each of the sub-models to the student data, obtaining an
AIC value.  Finally, we find the Akaike weighs for each of the
sub-models.  The Akaike weights give the relative probability that learning
occurred at each step.

\begin{figure}
  \centering \includegraphics{step-weights.eps}
   \caption{Akaike weights for the submodels $P_\mathrm{step,L}(j)$.  
     This gives the relative probability that
      the student learned the KC just before step $L$.}
    \label{step-weights}
\end{figure}

Let us illustrate this technique with a simple example. 
Suppose the bit string for a particular student-KC pair is 
00011011 (8 opportunities); see Fig.~\ref{step-weights}.
We fit this datum to 8 sub-models of the step model, corresponding to
$L\in\{1,2,\ldots,8\}$, by maximizing the log likelihood $\log\mathcal{L}$.  
The associated AIC values are
given by $\mathrm{AIC}_L=2 K-\log \mathcal{L}$  where $K$ is the
number of fit parameters.  Note that there are two parameters 
($s$ and $g$) when $L>1$ and there is only one parameter ($s$) when $L=1$.
%
%\begin{table}
%\caption{
%\begin{tabular}{crrrrrrrr}
% opportunity & 1 & 2 & 3 &4 & 5 & 6 & 7 & 8 \\
 % AIC &   13.1 & 13.6 & 11.6 & 9.0 & 13.0 & 14.5 & 11.6 & 13.6 \\
%\end{tabular}
%\end{equation}
%
Not suprisingly, the best fit (lowest AIC) corresponds to the first
``1'' in the bit string at step~4.  From the AICs, we calculate 
the Akaike weights
%
\begin{equation}
     w_L=\frac{e^{-\mathrm{AIC}_L/2} }{\sum_{L^\prime}
       e^{-\mathrm{AIC}_{L^\prime}/2}} \; .
\end{equation}
%
The Akaike weight $w_L$ gives the relative probability that sub-model 
$P_{\mathrm{step},L}(j)$ is, of all the sub-models, the closest to the 
the model that actually generated the data.


Note that the case $L=1$ corresponds to the student having 
``learned the skill'' some time before the first step.  That is to say, 
the student does not acquire the skill while using the tutor system.
Thus, $w_1$ should be interpreted as the relative probability
that no learning has occurred while using the tutor system.

\section{Weighted gain}

Our ultimate goal is to distinguish steps that result in 
learning from steps that do not.  Hopefully, one can use this
information to infer something about the effectiveness of the help
given on a particular step, or the effectiveness of
the student activity on that step.

\begin{figure}
  \centering \includegraphics{weighted-gain-histogram.eps}
   \caption{Histogram of weighted gains $w_L \Delta_L$ for
     all steps in all student-KC pairs in student dataset $\mathcal{A}$.}
    \label{weighted-gain-histogram}
\end{figure}

It is not sufficient to know {\it when} learning has occurred but 
one must also determine {\it how much} learning has occurred.  
Consider the bit sequence 11011000.   When fit to the step model,  
the best fit will occur at $L=6$ but this would correspond to a
decrease in student performance for that skill.  In other cases, the 
change in student performance may be almost zero.  
In order to take this into account, we propose 
using the Akaike weight $w_L$ times the associated learning gain $\Delta_L$
to characterize a step.
We define the learning gain $\Delta_L=1-\hat{g}-\hat{s}$ where $\hat{g}$ and $\hat{s}$
are the Maximum Likelihood estimators for $g$ and $s$ given
by submodel $P_{\mathrm{step},L}(j)$.   For the ``no learning''
case $L=1$, we set $\Delta_1=0$.
We will call  $w_L \Delta_L$ the ``weighted gain'' associated with 
$P_{\mathrm{step},L}(j)$.
A histogram of $w_L \Delta_L$ for student dataset $\mathcal{A}$ is
shown in Fig.~\ref{weighted-gain-histogram}.  Note that the vast
majority of steps (29730) have almost zero weighted gain.  
We also see that there
is a significant number of steps with negative gain (988),
but there are somewhat more steps with positive gain (1312) .

The fact that there are so many steps with negative gain is
symptomatic of bit strings that are very noisy (a lot of
randomness).  Indeed, if we compare the histogram for student
dataset $\mathcal{A}$ with the histogram for a randomly 
generated dataset $\mathcal{R}$ (we take $\mathcal{A}$ and
randomly permute the steps) we find a similar distribution;
see Fig.~\ref{weighted-gain-histogram2}.

What would the distribution look like if the data weren't 
so noisy?  To see this, we generated an artificial ``ideal'' dataset
$\mathcal{I}$ where there were no slips or guesses, but having
the same bit string length distribution as $\mathcal{A}$ 
(Fig.~\ref{student-length-histogram}).  Thus, the bit strings
in $\mathcal{I}$ have the form $00\cdots011\cdots1$.
In this case, for each student-KC pair, we expect a single 
large weighted gain (corresponding to the first 1 in the bit string) 
and the remaining weighted gains to be nearly zero.  The resulting 
distribution of gains is shown
in  Fig.~\ref{weighted-gain-histogram2}.

\begin{figure}
  \centering \includegraphics{weighted-gain-histogram2.eps}
   \caption{Histogram of weighted gains $w_L \Delta_L$ for
     the student dataset $\mathcal{A}$, 
     a randomly generated dataset $\mathcal{R}$,
     and an artificial ideal dataset $\mathcal{I}$.}
    \label{weighted-gain-histogram2}
\end{figure}


We propose to use the following average of the weighed gains as
a ``quality index'' for determining how suitable a 
dataset is for determining the point of learning for an individual
student-KC pair:
%
\begin{equation}
           Q= \frac{1}{N} \sum_\alpha \sum_L w_L \Delta_L
\end{equation}
%
where $\alpha$ is an index running over all student-KC pairs in a 
dataset and $N$ is the number of student-KC pairs.
For the random dataset $\mathcal{R}$, we expect the distribution of 
weighted gains to be symmetric about zero and thus $Q$ to approach zero.  
Numerically, we get  $Q=-0.002\pm0.002$, consistent with zero.
For the ``ideal'' dataset $\mathcal{I}$,
we expect, for the first 1 in the bit string, $w_L$ to be nearly one 
with associated $\Delta_L$ also nearly one so that $Q\to 1$ in
the limit of many opportunities.
Numerically, we get $Q=0.5240\pm0.0003$, which is due to the 
large number of student-KC pairs having just a few steps.  
For the student dataset $\mathcal{A}$, we obtain $Q=0.047\pm0.006$, which
is small, but significant ($p<0.001$). Thus, we conclude that the
student dataset is of sufficient quality to use in determining where learning has occurred.


\section{Conclusion}

One thing that we can conclude is that determining the
moment of learning for an individual student is a murky
business, as can be seen by comparing the weighted gains for
the student dataset to the weighted gains of a 
randomly generated dataset.

However, the fact that we can find significant positive average gains,
suggests the situation is not hopeless.  We can use $Q$ to determine
the predictive power of a given dataset. 


% Bibliography
\bibliographystyle{acmlarge}
\bibliography{education-modeling}



\end{document}